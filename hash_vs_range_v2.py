import os
from openai import OpenAI

# Initialize client correctly
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

schema = """
CREATE TABLE hash_vs_range (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   PRIMARY KEY(id HASH)
) SPLIT INTO 3 TABLETS;
"""

query = "select * from hash_vs_range where id > 1 and id < 4;"

explain_plan = """
explain (analyze, dist) select * from hash_vs_range where id > 1 and id < 4;
                                                 QUERY PLAN
--------------------------------------------------------------------------------------------
 Seq Scan on hash_vs_range  (cost=0.00..105.00 rows=1000 width=54) (actual time=0.463..0.465 rows=2 loops=1)
   Storage Filter: ((id > 1) AND (id < 4))
   Storage Table Read Requests: 1
   Storage Table Read Execution Time: 0.392 ms
   Storage Table Rows Scanned: 100
 Planning Time: 1.860 ms
 Execution Time: 0.517 ms
 Storage Read Requests: 1
 Storage Read Execution Time: 0.392 ms
 Storage Rows Scanned: 100
 Storage Write Requests: 0
 Catalog Read Requests: 5
 Catalog Read Execution Time: 1.434 ms
 Catalog Write Requests: 0
 Storage Flush Requests: 0
 Storage Execution Time: 1.826 ms
 Peak Memory Usage: 24 kB

"""

context = """
You are a PostgreSQL query tuning expert working on YugabyteDB distributed SQL.

Your job is to analyze queries that may have range predicates and ORDER BY clauses running inefficiently on hash-partitioned tables. You will be given:
- SQL statement
- Explain plan output
- Table schema

The main problem occurs when:
- The query contains WHERE predicates like: field > value, field < value, field BETWEEN value1 AND value2, or ORDER BY field.
- The table is hash-partitioned.
- The explain plan shows sequential scans.
- There is no ASC index to support the range predicate or ORDER BY.
- If there *is* an ASC index, but a sequential scan still occurs, indicate that this suggests a problem in query planner or index usability.

---

YugabyteDB Partitioning Model:

- YugabyteDB partitions tables into tablets.
- By default, tables are HASH partitioned on primary keys for even write distribution but inefficient range access.
- RANGE partitioning (PRIMARY KEY(field ASC)) keeps rows ordered but may create write hotspots for sequential inserts.

Common query problem:

- Queries using range predicates or ORDER BY on hash-partitioned tables often trigger sequential scans, even when only a small key range is needed.
- The lack of ordered storage leads to full table scans.

---

You must always generate your output following this report format:

==============================
Report

Your input query:

{SQL_STATEMENT}

Problem Detected:

Explain that this query uses a range predicate and/or ORDER BY. The explain plan shows sequential scan on a hash-partitioned table without a supporting ASC index.

Proposed Solutions:

Solution 1 - Add Secondary ASC Index

Propose creating an ASC index to optimize the query:
CREATE INDEX table_field_idx ON table_name (field ASC);

Explain that this allows efficient range scans. Drawbacks include write overhead and potential hotspots on monotonically increasing keys.

---

Solution 2 - Repartition Table to Range Partitioning

Propose recreating the table with ASC primary key to directly store data ordered by key:

CREATE TABLE table_v2 (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   PRIMARY KEY (id ASC)
) SPLIT INTO N TABLETS;

Explain that this avoids full scans for range queries but may cause write hotspots. Mention that recreating large tables can be operationally difficult.

---

Solution 3 - Use Secondary Index with Bucket ID

Propose adding virtual bucketing to secondary index:

CREATE INDEX table_bucket_idx ON table_name (
  (yb_hash_code(field) % 3) ASC, 
  field
);

Show how queries would need UNION ALL across buckets:

ELECT * FROM (
 (SELECT * FROM table_name WHERE yb_hash_code(field) % 3 = 0 ORDER BY id ASC LIMIT 3)
 UNION ALL
 (SELECT * FROM table_name WHERE yb_hash_code(field) % 3 = 1 ORDER BY id ASC LIMIT 3)
 UNION ALL
 (SELECT * FROM table_name WHERE yb_hash_code(field) % 3 = 2 ORDER BY id ASC LIMIT 3)
) AS combined
ORDER BY field ASC LIMIT 3;


Explain that this reduces hotspots while retaining the original table structure, but requires modifying queries for ORDER BY.

---

Solution 4 - Modify Table Primary Key to Include Bucket ID

Propose modifying the base table schema to include bucket_id directly in the primary key:

CREATE TABLE table_v3 (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   bucketid smallint DEFAULT ((random()*10)::int % 3), 
   PRIMARY KEY (bucketid ASC, id ASC)
) SPLIT INTO 3 TABLETS;

Queries must be rewritten as:

SELECT * FROM (
 (SELECT * FROM table_v3 WHERE bucketid = 0 AND ORDER BY id ASC LIMIT 3)
 UNION ALL
 (SELECT * FROM table_v3 WHERE bucketid = 1 AND ORDER BY id ASC LIMIT 3)
 UNION ALL
 (SELECT * FROM table_v3 WHERE bucketid = 2 AND ORDER BY id ASC LIMIT 3)
) AS combined
ORDER BY id ASC LIMIT 3;

Explain that this reduces hotspots but requires full table rebuild and query rewrites.

---

Conclusion:

Summarize the trade-offs:
- Secondary indexes are simplest but may introduce write overhead and hotspots.
- Bucketed indexes reduce hotspots but complicate ORDER BY queries.
- Repartitioning tables is ideal for pure range workloads but requires downtime or ETL reload.
==============================

Always output the full SQL examples for each solution.
If any solution is not applicable based on explain plan and schema, write 'N/A' for that section.
"""



prompt = f"""

You are a postgres performance expert. Here you will be analyzing a postgres comaptibale database Yugabyte.

Please review and optimize the following PostgreSQL sql query based on the schema definition and explain plan.

Here is the query: {query}
Here is the schema of the table: {schema}
Here is the explain plan for the query were are running:  {explain_plan}


{context}

Output the example SQL from customer at the beginning of report
"""

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a PostgreSQL query tuning expert."},
        {"role": "user", "content": prompt}
    ]
)

print(response.choices[0].message.content)

