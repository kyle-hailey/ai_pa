# ai_pa


The given SQL query is:
```
select * from hash_vs_range where id > 1 and id < 4;
```
This query is a range query which is performing a scan on the 'hash_vs_range' table where primary key is 'id'. Since this table is HASH partitioned, the above range query will lead to full table scans across all the tablets, resulting in sub-optimal performance.

Solution 1: Add Secondary ASC Index:
```
CREATE INDEX hash_vs_range_idx ON hash_vs_range (id ASC);
```
This approach will create an index on 'id' in ascending order, which will make range scans more efficient.

Pros:
- The range queries and ORDER BY clauses will be faster without requiring query rewrites.
Cons:
- Adds overhead for INSERT, UPDATE, DELETE commands as these should now maintain the index.
- Potential write hotspots if 'id' column values are monotonically increasing.

Solution 2: Modify The Table to Range Partitioned:
```
CREATE TABLE hash_vs_range (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   PRIMARY KEY(id ASC)
) SPLIT INTO 3 TABLETS;
```
This table is now created to be range partitioned, making range queries and order by operations more efficient.

Pros:
- Supports range and order by queries transparently without requiring secondary indexes.
Cons:
- Recreating the table might be challenging for large tables or tables that are heavily used.
- If the indexed column has a monotonically increasing pattern, write hotspots may occur.

Solution 3: Secondary Index with Bucket ID
```
CREATE INDEX hash_vs_range_bucket_idx ON hash_vs_range ((yb_hash_code(id) % 3) ASC, id ASC);
```
This solution helps to spread inserts across different buckets, reducing potential write hotspots. However, for ORDER BY queries, each bucket must be individually queried with 'UNION ALL'.

Pros:
- Balances the data distribution on the secondary index.
Cons:
- Adds overhead for maintaining a secondary index.
- Order by queries need to be rewritten to take advantage of this structure.

Order By Rewrite Example:
```
SELECT * FROM (
    (SELECT * FROM hash_vs_range WHERE (yb_hash_code(id) % 3) = 0 ORDER BY id ASC LIMIT 3) UNION ALL
    (SELECT * FROM hash_vs_range WHERE (yb_hash_code(id) % 3) = 1 ORDER BY id ASC LIMIT 3) UNION ALL
    (SELECT * FROM hash_vs_range WHERE (yb_hash_code(id) % 3) = 2 ORDER BY id ASC LIMIT 3)
) AS combined ORDER BY id ASC LIMIT 3;
```

Solution 4: Modifying the table schema to use bucket_id:
```
CREATE TABLE hash_vs_range (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   bucket_id smallint DEFAULT (random()::smallint % 3),
   PRIMARY KEY(bucket_id ASC, id ASC)
) SPLIT INTO 3 TABLETS;
```
This approach helps to remove the need for secondary index for data distribution.

Pros:
- Removes the need for maintaining a secondary index.
Cons:
- Requires a full table rebuild, which may not be practical for large production systems.

Order By Rewrite with UNION ALL and bucket_id:
```
SELECT * FROM (
    (SELECT * FROM hash_vs_range WHERE bucket_id = 0 ORDER BY id ASC LIMIT 3) UNION ALL
    (SELECT * FROM hash_vs_range WHERE bucket_id = 1 ORDER BY id ASC LIMIT 3) UNION ALL
    (SELECT * FROM hash_vs_range WHERE bucket_id = 2 ORDER BY id ASC LIMIT 3)
) AS combined ORDER BY id ASC LIMIT 3;
```

These are the possible optimizations for the provided SQL query and their respective pros and cons. The final decision should be made based on additional factors such as your workload, data distribution, and infrastructure.


* V2


Report

Your input query:

```sql
select * from hash_vs_range where id > 1 and id < 4;
```

Problem Detected:

This query uses a range predicate on the id field. According to the explain plan, a sequential scan was performed on the hash-partitioned table `hash_vs_range`. As there is no ASC index on the id field, these range queries can lead to inefficient full table scans.

Proposed Solutions:

Solution 1 - Add Secondary ASC Index

We can create an ASC index to optimize the query:

```sql
CREATE INDEX hash_vs_range_id_idx ON hash_vs_range(id ASC);
```

This index allows the database to efficiently scan only the necessary range of the id field, avoiding full table scans. However, it introduces the overhead of maintaining the index during writes and may create write hotspots if the id values increase monotonically.

---

Solution 2 - Repartition Table to Range Partitioning

Another option is to create a new version of the table with an ASC primary key:

```sql
CREATE TABLE hash_vs_range_v2 (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   PRIMARY KEY (id ASC)
) SPLIT INTO 3 TABLETS;
```

This will make the id field stored in order, which will improve the performance of these kinds of range queries. It reduces the likelihood of full table scans but may result in uneven data distribution causing write hotspots. However, migrating data to a new table can be operationally challenging for large tables.

---

Solution 3 - Use Secondary Index with Bucket ID

Creating a secondary index with virtual bucketing may alleviate the hotspot problem while retaining query efficiency:

```sql
CREATE INDEX hash_vs_range_bucket_idx ON hash_vs_range (
  (yb_hash_code(id) % 3) ASC, 
  id
);
```

However, this approach requires modifying the original query:

```sql
SELECT * FROM (
  (SELECT * FROM hash_vs_range WHERE yb_hash_code(id) % 3 = 0 AND id > 1 AND id < 4 ORDER BY id ASC)
  UNION ALL
  (SELECT * FROM hash_vs_range WHERE yb_hash_code(id) % 3 = 1 AND id > 1 AND id < 4 ORDER BY id ASC)
  UNION ALL
  (SELECT * FROM hash_vs_range WHERE yb_hash_code(id) % 3 = 2 AND id > 1 AND id < 4 ORDER BY id ASC)
) AS combined
ORDER BY id ASC;
```
---

Solution 4 - Modify Table Primary Key to Include Bucket ID

Modifying the base table schema to include `bucket_id` directly in the primary key:

```sql
CREATE TABLE hash_vs_range_v3 (
   id bigint GENERATED BY DEFAULT AS IDENTITY,
   car varchar,
   speed int,
   ts timestamp,
   bucketid smallint DEFAULT ((random()*10)::int % 3), 
   PRIMARY KEY (bucketid ASC, id ASC)
) SPLIT INTO 3 TABLETS;
```

Queries would be rewritten as:

```sql
SELECT * FROM (
 (SELECT * FROM hash_vs_range_v3 WHERE bucketid = 0 AND id > 1 AND id < 4 ORDER BY id ASC)
 UNION ALL
 (SELECT * FROM hash_vs_range_v3 WHERE bucketid = 1 AND id > 1 AND id < 4 ORDER BY id ASC)
 UNION ALL
 (SELECT * FROM hash_vs_range_v3 WHERE bucketid = 2 AND id > 1 AND id < 4 ORDER BY id ASC)
) AS combined
ORDER BY id ASC;
```

This approach requires a full table rebuild and rewriting the original query.

---

Conclusion:

The best solution depends on the specific use case:
- Adding a secondary index is the simplest solution but may introduce write overhead.
- Repartitioning the table into range partitions is efficient for range queries but may cause write hotspots and requires significant operational effort.
- Using a bucketed index or modifying the table to include a bucket id in the primary key can help balance the load, but complicates queries and may require rewriting the application's SQL queries.


### three table join

Report

Your input query:

```
SELECT *
FROM customers_tmp c
JOIN orders_tmp o ON c.id = o.customer_id
JOIN products_tmp p ON o.id = p.id
WHERE o.date > '2024-01-15'
  AND p.category = 'Shoes';
```
Problem Detected:

The query uses a range predicate by checking where `o.date > '2024-01-15'`. The explain plan refers to a sequential scan on a hash-partitioned table `orders_tmp` whilst checking the date which may not be efficient due to the lack of an ASC index supporting this range operation. 

Proposed Solutions:

Solution 1 - Add Secondary ASC Index

A workaround for this would be to create an ASC index to optimize the query:

```sql
CREATE INDEX orders_tmp_date_idx ON orders_tmp (date ASC);
```
This allows more efficient range scans of the date values. However, it's worth noting that each write operation will now have an additional overhead due to the need to keep the index up-to-date which could potentially create hotspots on monotonically increasing keys (such as dates).

---

Solution 2 - Repartition Table to Range Partitioning

A second solution would be to recreate the `orders_tmp` table with the `date` as the ASC primary key which means data is written to storage in an ordered manner:

```sql
CREATE TABLE orders_tmp_v2 (
   id SERIAL
   customer_id INT,
   date DATE,
   PRIMARY KEY (date ASC)
) SPLIT INTO N TABLETS;
```
This will avoid full scans when executing range queries, but it's important to remember that this could cause write hotspots due to writes being concentrated on a particular tablet. Recreating large tables could also be operationally complex and require substantial downtime to implement. 

---

Solution 3 - Use Secondary Index with Bucket ID

Another solution is to add virtual bucketing to the secondary index:

```sql
CREATE INDEX orders_bucket_idx ON orders_tmp (
  (yb_hash_code(date) % 3) ASC, 
  date ASC
);
```
Queries would then need rewriting to a UNION ALL across buckets:

```sql
SELECT * FROM (
 (SELECT * FROM orders_tmp WHERE yb_hash_code(date) % 3 = 0 AND date > '2024-01-15' ORDER BY date ASC)
 UNION ALL
 (SELECT * FROM orders_tmp WHERE yb_hash_code(date) % 3 = 1 AND date > '2024-01-15' ORDER BY date ASC)
 UNION ALL
 (SELECT * FROM orders_tmp WHERE yb_hash_code(date) % 3 = 2 AND date > '2024-01-15' ORDER BY date ASC)
) AS combined
ORDER BY date ASC;
```
This method may reduce hotspots while retaining the original table structure, but it will require modifying existing queries.

---

Solution 4 - Modify Table Primary Key to Include Bucket ID

The final suggestion is to modify the base table schema to include the bucket_id directly in the primary key:

```sql
CREATE TABLE orders_tmp_v3 (
   id SERIAL,
   customer_id INT,
   date DATE,
   bucketid smallint DEFAULT ((random()*10)::int % 3), 
   PRIMARY KEY (bucketid ASC, date ASC)
) SPLIT INTO 3 TABLETS;
```
Queries must be rewritten in a similar way to solution 3:

```sql
SELECT * FROM (
 (SELECT * FROM orders_tmp_v3 WHERE bucketid = 0 AND date > '2024-01-15' ORDER BY date ASC)
 UNION ALL
 (SELECT * FROM orders_tmp_v3 WHERE bucketid = 1 AND date > '2024-01-15' ORDER BY date ASC)
 UNION ALL
 (SELECT * FROM orders_tmp_v3 WHERE bucketid = 2 AND date > '2024-01-15' ORDER BY date ASC)
) AS combined
ORDER BY date ASC;
```
While this solution reduces hotspots it also implies a complete rebuild of the table and the rewriting of associated queries.

---

Conclusion:

Each solution comes with its own set of trade-offs. While secondary indexes provide a straightforward solution, they also introduce potential write complexity and hotspots. Bucketed indexes can reduce these hotspots while maintaining the base table structure, but complicate query writing with a need for unions across buckets. A total repartitioning of the table is ideal for workloads that heavily feature range operations, but can also create hotspots and requires significant downtime or ETL to implement. Therefore, the choice of solution should carefully consider these factors.
